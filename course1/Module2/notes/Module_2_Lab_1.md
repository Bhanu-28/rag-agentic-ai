# Module 2 Lab 1

LangChain is an open-source framework designed to develop applications that leverage large language models (LLMs). LangChain stands out by providing essential tools and abstractions that enhance the customization, accuracy, and relevance of the information generated by these models.

LangChain offers a generic interface compatible with nearly any LLM. This generic interface facilitates a centralized development environment so that data scientists can seamlessly integrate LLM-powered applications with external data sources and software workflows. This integration is crucial for organizations looking to harness AI's full potential in their processes.

One of LangChain's most powerful features is its module-based approach. This approach supports flexibility when performing experiments and the optimization of interactions with LLMs. Data scientists can dynamically compare prompts and switch between foundation models without significant code modifications. These capabilities save valuable development time and enhance the developer's ability to fine-tune applications.

# **Setup**

For this lab, you will use the following libraries:

- [`ibm-watson-ai`, `ibm-watson-machine-learning`](https://ibm.github.io/watson-machine-learning-sdk/index.html) for using LLMs from IBM's watsonx.ai.
- [`langchain`, `langchain-ibm`, `langchain-community`, `langchain-experimental`](https://www.langchain.com/) for using relevant features from LangChain.
- [`pypdf`](https://pypi.org/project/pypdf/) is an open-source pure-python PDF library capable of splitting, merging, cropping, and transforming the pages of PDF files.
- [`chromadb`](https://www.trychroma.com/) is an open-source vector database used to store embeddings.

Requirements

```bash
%%capture
!pip install --force-reinstall --no-cache-dir tenacity==8.2.3 --user
!pip install "ibm-watsonx-ai==1.0.8" --user
!pip install "ibm-watson-machine-learning==1.0.367" --user
!pip install "langchain-ibm==0.1.7" --user
!pip install "langchain-community==0.2.10" --user
!pip install "langchain-experimental==0.0.62" --user
!pip install "langchainhub==0.1.18" --user
!pip install "langchain==0.2.11" --user
!pip install "pypdf==4.2.0" --user
!pip install "chromadb==0.4.24" --user
pip install ipdb
```

A large language model (LLM) serves as the interface for the AI's capabilities. The LLM processes plain text input and generates text output, forming the core functionality needed to complete various tasks. When integrated with LangChain, the LLM becomes a powerful tool, providing the foundational structure necessary for building and deploying sophisticated AI applications.

Changed the model, as it is not working properly.

### # model_id = 'meta-llama/llama-3-3-70b-instruct'

### New → model_id = 'meta-llama/llama-3-405b-instruct'

### Chat model

Chat models support assigning distinct roles to conversation messages, helping to distinguish messages from AI, users, and instructions such as system messages.

To enable the LLM from watsonx.ai to work with LangChain, you need to wrap the LLM using `WatsonLLM()`. This wrapper converts the LLM into a chat model, which allows the LLM to integrate seamlessly with LangChain's framework for creating interactive and dynamic AI applications.

**Chat message**

The chat model takes a list of messages as input and returns a new message. All messages have both a role and a content property. Here's a list of the most commonly used types of messages:

- `SystemMessage`: Use this message type to prime AI behavior. This message type is usually passed in as the first in a sequence of input messages. (Example : you are pro cricket trainer.)
- `HumanMessage`: This message type represents a message from a person interacting with the chat model.
- `AIMessage`: This message type, which can be either text or a request to invoke a tool, represents a message from the chat model.

You can find more message types at [LangChain built-in message types](https://python.langchain.com/v0.2/docs/how_to/custom_chat_model/#messages).

## **Prompt templates**

Prompt templates help translate user input and parameters into instructions for a language model. You can use prompt templates to guide a model's response, helping the model understand the context and generate relevant and coherent language-based output.

Next, explore several different types of prompt templates.

**String prompt templates**

Use these prompt templates to format a single string. These templates are generally used for simpler inputs.

**from** langchain_core.prompts **import** PromptTemplate

Then, create a prompt template with variables for customization. We also create a dictionary to store inputs that will replace the placeholders. The keys match the variable names in the template, and values are what will be inserted.

### **String prompt templates**

Use these prompt templates to format a single string. These templates are generally used for simpler inputs.

```bash
**from** langchain_core.prompts **import** PromptTemplate
```

Then, create a prompt template with variables for customization. We also create a dictionary to store inputs that will replace the placeholders. The keys match the variable names in the template, and values are what will be inserted.

```bash
prompt **=** PromptTemplate.from_template("Tell me one {adjective} joke about {topic}")
input_ **=** {"adjective": "funny", "topic": "cats"}  *# create a dictionary to store the corresponding input to placeholders in prompt template*
```

Finally, format the prompt template with the input dictionary. The code below invokes the prompt with our input values, replacing {adjective} with "funny" and {topic} with "cats." The result will be a formatted string: "Tell me one funny joke about cats.”

```bash
prompt.invoke(input_)

Output : StringPromptValue(text='Tell me one funny joke about cats')
```

### **Chat prompt templates**

You can use these prompt templates to format a list of messages. These "templates" consist of lists of templates.

```bash
# Import the ChatPromptTemplate class from langchain_core.prompts module
from langchain_core.prompts import ChatPromptTemplate

# Create a ChatPromptTemplate with a list of message tuples
# Each tuple contains a role ("system" or "user") and the message content
# The system message sets the behavior of the assistant
# The user message includes a variable placeholder {topic} that will be replaced later
prompt = ChatPromptTemplate.from_messages([
 ("system", "You are a helpful assistant"),
 ("user", "Tell me a joke about {topic}")
])

# Create a dictionary with the variable to be inserted into the template
# The key "topic" matches the placeholder name in the user message
input_ = {"topic": "cats"}

# Format the chat template with our input values
# This replaces {topic} with "cats" in the user message
# The result will be a formatted chat message structure ready to be sent to a model
prompt.invoke(input_)

OutPut : ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), HumanMessage(content='Tell me a joke about cats')])
```

### **MessagesPlaceholder**

You can use the MessagesPlaceholder prompt template to add a list of messages in a specific location. In `ChatPromptTemplate.from_messages`, you saw how to format two messages, with each message as a string. But what if you want the user to supply a list of messages that you would slot into a particular spot? You can use `MessagesPlaceholder` for this task.

```bash
# Import MessagesPlaceholder for including multiple messages in a template
from langchain_core.prompts import MessagesPlaceholder
# Import HumanMessage for creating message objects with specific roles
from langchain_core.messages import HumanMessage

# Create a ChatPromptTemplate with a system message and a placeholder for multiple messages
# The system message sets the behavior for the assistant
# MessagesPlaceholder allows for inserting multiple messages at once into the template
prompt = ChatPromptTemplate.from_messages([
("system", "You are a helpful assistant"),
MessagesPlaceholder("msgs")  # This will be replaced with one or more messages
])

# Create an input dictionary where the key matches the MessagesPlaceholder name
# The value is a list of message objects that will replace the placeholder
# Here we're adding a single HumanMessage asking about the day after Tuesday
input_ = {"msgs": [HumanMessage(content="What is the day after Tuesday?"),
                   AIMessage(content = "Saturday" ),
                   HumanMessage(content = "No it is wrong...")
                  ]}

# Format the chat template with our input dictionary
# This replaces the MessagesPlaceholder with the HumanMessage in our input
# The result will be a formatted chat structure with a system message and our human message
prompt.invoke(input_)

Output : ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant'), 
				 HumanMessage(content='What is the day after Tuesday?'), AIMessage(content='Saturday'), HumanMessage(content='No it is wrong...')])
```

You can wrap the prompt and the chat model and pass them into a chain, which can invoke the message.

```bash
chain = prompt | llama_llm
response = chain.invoke(input = input_)
print(response)

Output : AI: I made a mistake. The day after Tuesday is Wednesday.
```

**Output parsers**

Output parsers take the output from an LLM and transform that output to a more suitable format. Parsing the output is very useful when you are using LLMs to generate any form of structured data, or to normalize output from chat models and other LLMs.

LangChain has lots of different types of output parsers. This is a [list](https://python.langchain.com/v0.2/docs/concepts/#output-parsers) of output parsers LangChain supports. In this lab, you will use the following two output parsers as examples:

- `JSON`: Returns a JSON object as specified. You can specify a Pydantic model and it will return JSON for that model. Probably the most reliable output parser for getting structured data that does NOT use function calling.
- `CSV`: Returns a list of comma-separated values.

**JSON parser**

This output parser allows users to specify an arbitrary JSON schema and query LLMs for outputs that conform to that schema.

```bash
# Import the JsonOutputParser from langchain_core to convert LLM responses into structured JSON
from langchain_core.output_parsers import JsonOutputParser
# Import BaseModel and Field from langchain_core's pydantic_v1 module
from langchain_core.pydantic_v1 import BaseModel, Field
```

JSON Parser

```bash
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate

# Create your JSON parser
json_parser = JsonOutputParser()

# Create the format instructions
format_instructions = """RESPONSE FORMAT: Return ONLY a single JSON object—no markdown, no examples, no extra keys.  It must look exactly like:
{
  "title": "movie title",
  "director": "director name",
  "year": 2000,
  "genre": "movie genre"
}

IMPORTANT: Your response must be *only* that JSON.  Do NOT include any illustrative or example JSON."""

# Create prompt template with instructions
prompt_template = PromptTemplate(
    template="""You are a JSON-only assistant.

Task: Generate info about the movie "{movie_name}" in JSON format.

{format_instructions}
""",
    input_variables=["movie_name"],
    partial_variables={"format_instructions": format_instructions},
)

# Create the chain
movie_chain = prompt_template | llama_llm |  json_parser

# Test with a movie name
movie_name = "The Matrix"
result = movie_chain.invoke({"movie_name":movie_name})

# Print the structured result
print("Parsed result:")
print(f"Title: {result['title']}")
print(f"Director: {result['director']}")
print(f"Year: {result['year']}")
print(f"Genre: {result['genre']}")
```

`Parsed result:
Title: The Matrix
Director: The Wachowskis
Year: 1999
Genre: Science Fiction`

### CSV Parser

```bash
# Import the CommaSeparatedListOutputParser to parse LLM responses into Python lists
from langchain.output_parsers import CommaSeparatedListOutputParser

# Create an instance of the parser that will convert comma-separated text into a Python list
output_parser = CommaSeparatedListOutputParser()

# Get formatting instructions that will tell the LLM how to structure its response
# These instructions explain to the LLM that it should return items in a comma-separated format
format_instructions = output_parser.get_format_instructions()

print(format_instructions)

# Create a prompt template that:
# 1. Instructs the LLM to answer the user query
# 2. Includes format instructions so the LLM knows to respond with comma-separated values
# 3. Asks the LLM to list five items of the specified subject
prompt = PromptTemplate(
    template="Answer the user query. {format_instructions}\nList five {subject}.",
    input_variables=["subject"],  # This variable will be provided when the chain is invoked
    partial_variables={"format_instructions": format_instructions},  # This variable is set once when creating the prompt
)

# Build a processing chain that:
# 1. Takes the subject and formats it into the prompt template
# 2. Sends the formatted prompt to the Llama LLM
# 3. Parses the LLM's response into a Python list using the CommaSeparatedListOutputParser
chain = prompt | llama_llm | output_parser

# Invoke the processing chain with "ice cream flavors" as the subject
# This will:
# 1. Substitute "ice cream flavors" into the prompt template
# 2. Send the formatted prompt to the Llama LLM
# 3. Parse the LLM's comma-separated response into a Python list
chain.invoke({"subject": "ice cream flavors"})
```

### **Documents**

**Document object**

A `Document` object in `LangChain` contains information about some data. A Document object has the following two attributes:

- `page_content`: `*str*`: This attribute holds the content of the document.
- `metadata`: `*dict*`: This attribute contains arbitrary metadata associated with the document. You can use the metadata to track various details, such as the document ID, the file name, and other details.

Let's examine how to create a `Document` object. `LangChain` uses the `Document` object type to handle text or documents.

```bash
# Import the Document class from langchain_core.documents module
# Document is a container for text content with associated metadata
from langchain_core.documents import Document

# Create a Document instance with:
# 1. page_content: The actual text content about Python
# 2. metadata: A dictionary containing additional information about this document
Document(page_content="""Python is an interpreted high-level general-purpose programming language.
 Python's design philosophy emphasizes code readability with its notable use of significant indentation.""",
metadata={
    'my_document_id' : 234234,                      # Unique identifier for this document
    'my_document_source' : "About Python",          # Source or title information
    'my_document_create_time' : 1680013019          # Unix timestamp for document creation (March 28, 2023)
 })
```

### Document Loaders

 in LangChain are designed to load documents from a variety of sources; for instance, loading a PDF file and having the LLM read the PDF file using LangChain.

LangChain offers over 100 distinct document loaders, along with integrations with other major providers, such as AirByte and Unstructured. These integrations enable loading of all kinds of documents (HTML, PDF, code) from various locations including private Amazon S3 buckets, as well as from public websites).

You can find a list of document types that LangChain can load at [LangChain Document loaders](https://python.langchain.com/v0.1/docs/integrations/document_loaders/).

In this lab, you will use the PDF loader and the URL and website loader.

### **PDFLoader**

By using the PDF loader, you can load a PDF file as a `Document` object.

In this example, you will load the following paper about using LangChain. You can access and read the paper here: [Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model](https://doi.org/10.48550/arXiv.2403.05568).

```bash
# Import the PyPDFLoader class from langchain_community's document_loaders module
# This loader is specifically designed to load and parse PDF files
from langchain_community.document_loaders import PyPDFLoader

# Create a PyPDFLoader instance by passing the URL of the PDF file
# The loader will download the PDF from the specified URL and prepare it for loading
loader = PyPDFLoader("https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf")

# Call the load() method to:
# 1. Download the PDF if needed
# 2. Extract text from each page
# 3. Create a list of Document objects, one for each page of the PDF
# Each Document will contain the text content of a page and metadata including page number
document = loader.load()

document[2]  # take a look at the page 2

print(document[2].page_content[:1000])

```

### **URL and website loader**

You can also load content from a URL or website into a `Document` object:

```bash
# Import the WebBaseLoader class from langchain_community's document_loaders module
# This loader is designed to scrape and extract text content from web pages
from langchain_community.document_loaders import WebBaseLoader

# Create a WebBaseLoader instance by passing the URL of the web page to load
# This URL points to the LangChain documentation's introduction page
loader = WebBaseLoader("https://python.langchain.com/v0.2/docs/introduction/")

# Call the load() method to:
# 1. Send an HTTP request to the specified URL
# 2. Download the HTML content
# 3. Parse the HTML to extract meaningful text
# 4. Create a list of Document objects containing the extracted content
web_data = loader.load()

# Print the first 1000 characters of the page content from the first Document
# This provides a preview of the successfully loaded web content
# web_data[0] accesses the first Document in the list
# .page_content accesses the text content of that Document
# [:1000] slices the string to get only the first 1000 characters
print(web_data[0].page_content[:1000])
```

### **Text splitters**

After you load documents, you will often want to transform those documents to better suit your application.

One of the most simple examples of making documents better suit your application is to split a long document into smaller chunks that can fit into your model's context window. LangChain has built-in document transformers that ease the process of splitting, combining, filtering, and otherwise manipulating documents.

At a high level, here is how text splitters work:

1. They split the text into small, semantically meaningful chunks (often sentences).
2. They start combining these small chunks of text into a larger chunk until you reach a certain size (as measured by a specific function).
3. After the combined text reaches the new chunk's size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap to keep context between chunks.

For a list of types of text splitters LangChain supports, see [LangChain Text Splitters](https://python.langchain.com/v0.1/docs/modules/data_connection/document_transformers/).

Let's use a simple `CharacterTextSplitter` as an example of how to split the LangChain paper you just loaded.

This is the simplest method. This splits based on characters (by default "\n\n") and measures chunk length by number of characters.

`CharacterTextSplitter` is the simplest method of splitting the content. These splits are based on characters (by default "\n\n") and measures chunk length by number of characters.

```bash
# Import the CharacterTextSplitter class from langchain.text_splitter module
# Text splitters are used to divide large texts into smaller, manageable chunks
from langchain.text_splitter import CharacterTextSplitter

# Create a CharacterTextSplitter with specific configuration:
# - chunk_size=200: Each chunk will contain approximately 200 characters
# - chunk_overlap=20: Consecutive chunks will overlap by 20 characters to maintain context
# - separator="\n": Text will be split at newline characters when possible
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20, separator="\n")

# Split the previously loaded document (PDF or other text) into chunks
# The split_documents method:
# 1. Takes a list of Document objects
# 2. Splits each document's content based on the configured parameters
# 3. Returns a new list of Document objects where each contains a chunk of text
# 4. Preserves the original metadata for each chunk
chunks = text_splitter.split_documents(document)

# Print the total number of chunks created
# This shows how many smaller Document objects were generated from the original document(s)
# The number depends on the original document length and the chunk_size setting
print(len(chunks))

chunks[70].page_content   # take a look at any chunk's page content
```

### **Exercise 3**

### **Working with Document Loaders and Text Splitters**

You now know about about Document objects and how to load content from different sources. Now, let's implement a workflow to load documents, split them, and prepare them for retrieval.

**Instructions:**

1. Import the necessary document loaders to work with both PDF and web content.
2. Load the provided paper about LangChain architecture.
3. Create two different text splitters with varying parameters.
4. Compare the resulting chunks from different splitters.
5. Examine the metadata preservation across splitting.
6. Create a simple function to display statistics about your document chunks.

**Starter code: provide your solution in the TODO parts**

```bash
from langchain_core.documents import Document
from langchain_community.document_loaders import PyPDFLoader, WebBaseLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter

# Load the LangChain paper
paper_url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/96-FDF8f7coh0ooim7NyEQ/langchain-paper.pdf"
pdf_loader = PyPDFLoader(paper_url)
pdf_document = pdf_loader.load()

# Load content from LangChain website
web_url = "https://python.langchain.com/v0.2/docs/introduction/"
web_loader = WebBaseLoader(web_url)
web_document = web_loader.load()

# Create two different text splitters
splitter_1 = CharacterTextSplitter(chunk_size=300, chunk_overlap=30, separator="\n")
splitter_2 = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)##TODO  # Create a different splitter with different parameters

# Apply both splitters to the PDF document
chunks_1 = splitter_1.split_documents(pdf_document)
chunks_2 = splitter_2.split_documents(web_document)

# Define a function to display document statistics
def display_document_stats(docs, name):
    """Display statistics about a list of document chunks"""
    total_chunks = len(docs)
    total_chars = sum(len(doc.page_content) for doc in docs)
    avg_chunk_size = total_chars / total_chunks if total_chunks > 0 else 0
    
    # Count unique metadata keys across all documents
    all_metadata_keys = set()
    for doc in docs:
        all_metadata_keys.update(doc.metadata.keys())
    
    # Print the statistics
    print(f"\n=== {name} Statistics ===")
    print(f"Total number of chunks: {total_chunks}")
    print(f"Average chunk size: {avg_chunk_size:.2f} characters")
    print(f"Metadata keys preserved: {', '.join(all_metadata_keys)}")
    
    if docs:
        print("\nExample chunk:")
        example_doc = docs[min(5, total_chunks-1)]  # Get the 5th chunk or the last one if fewer
        print(f"Content (first 150 chars): {example_doc.page_content[:150]}...")
        print(f"Metadata: {example_doc.metadata}")
        
        # Calculate length distribution
        lengths = [len(doc.page_content) for doc in docs]
        min_len = min(lengths)
        max_len = max(lengths)
        print(f"Min chunk size: {min_len} characters")
        print(f"Max chunk size: {max_len} characters")

# Display stats for both chunk sets
display_document_stats(chunks_1, "Splitter 1")
display_document_stats(chunks_2, "Splitter 2")
```

### **Embedding models**

Embedding models are specifically designed to interface with text embeddings.

Embeddings generate a vector representation for a specified piece or "chunk" of text. Embeddings offer the advantage of allowing you to conceptualize text within a vector space. Consequently, you can perform operations such as semantic search, where you identify pieces of text that are most similar within the vector space.

IBM, Open AI, Hugging Face, and others offer embedding models. Here, you will use the embedding model from IBM's watsonx.ai to work with the text.

```bash

# Import the EmbedTextParamsMetaNames class from ibm_watsonx_ai.metanames module
# This class provides constants for configuring Watson embedding parameters
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames

# Configure embedding parameters using a dictionary:
# - TRUNCATE_INPUT_TOKENS: Limit the input to 3 tokens (very short, possibly for testing)
# - RETURN_OPTIONS: Request that the original input text be returned along with embeddings
embed_params = {
 EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
 EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
}
# Import the WatsonxEmbeddings class from langchain_ibm module
# This provides an integration between LangChain and IBM's Watson AI services
from langchain_ibm import WatsonxEmbeddings

# Create a WatsonxEmbeddings instance with the following configuration:
# - model_id: Specifies the "slate-125m-english-rtrvr-v2" embedding model from IBM
# - url: The endpoint URL for the Watson service in the US South region
# - project_id: The Watson project ID to use ("skills-network")
# - params: The embedding parameters configured earlier
watsonx_embedding = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr-v2",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="skills-network",
    params=embed_params,
)
texts = [text.page_content for text in chunks]

embedding_result = watsonx_embedding.embed_documents(texts)
embedding_result[0][:5]

#OutPut : 
[-0.011278366670012474,
 0.01716080866754055,
 0.0005690520629286766,
 -0.01606140471994877,
 -0.02355504222214222]
```

**Vector stores**

One of the most common ways to store and search over unstructured data is to embed the text data and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. You can use a [vector store](https://python.langchain.com/v0.1/docs/modules/data_connection/vectorstores/) to store embedded data and perform vector search for you.

You can find many vector store options. Here, the code uses `Chroma`.

```bash
from langchain.vectorstores import Chroma
**docsearch = Chroma.from_documents(chunks, watsonx_embedding)
query = "Langchain"
docs = docsearch.similarity_search(query)
print(docs[0].page_content)**
```

### **Retrievers**

A retriever is an interface that returns documents using an unstructured query. Retrievers are more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. You can still use vector stores as the backbone of a retriever. Note that other types of retrievers also exist.

Retrievers accept a string `query` as input and return a list of `Documents` as output.

You can view a list of the advanced retrieval types LangChain supports at [https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/)

A list of advanced retrieval types LangChain could support is available at [https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/). Let's introduce the `Vector store-backed retriever` and `Parent document retriever` as examples.

### **Vector store-backed retrievers**

Vector store retrievers are retrievers that use a vector store to retrieve documents. They are a lightweight wrapper around the vector store class to make it conform to the retriever interface. They use the search methods implemented by a vector store, such as similarity search and MMR (Maximum marginal relevance), to query the texts in the vector store.

Now that you have constructed a vector store `docsearch`, you can easily construct a retriever such as seen in the following code.

```bash
# Use the docsearch vector store as a retriever
# This converts the vector store into a retriever interface that can fetch relevant documents
retriever = docsearch.as_retriever()

# Invoke the retriever with the query "Langchain"
# This will:
# 1. Convert the query text "Langchain" into an embedding vector
# 2. Perform a similarity search in the vector store using this embedding
# 3. Return the most semantically similar documents to the query
docs = retriever.invoke("Langchain")

# Access the first (most relevant) document from the retrieval results
# This returns the full Document object including:
# - page_content: The text content of the document
# - metadata: Any associated metadata like source, page numbers, etc.
# The returned document is the one most semantically similar to "Langchain"
docs[0]
```

### Parent document retrievers

When splitting documents for retrieval, there are often conflicting goals:

You want small documents so their embeddings can most accurately reflect their meaning. If the documents are too long, then the embeddings can lose meaning.
You want to have long enough documents to retain the context of each chunk of text.
The ParentDocumentRetriever strikes that balance by splitting and storing small chunks of data. During retrieval, this retriever first fetches the small chunks, but then looks up the parent IDs for the data and returns those larger documents.

```bash
# Set up two different text splitters for a hierarchical splitting approach:

# 1. Parent splitter creates larger chunks (2000 characters)
# This is used to split documents into larger, more contextually complete sections
parent_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\n')

# 2. Child splitter creates smaller chunks (400 characters)
# This is used to split the parent chunks into smaller pieces for more precise retrieval
child_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=20, separator='\n')

# Create a Chroma vector store with:
# - A specific collection name "split_parents" for organization
# - The previously configured Watson embeddings function
vectorstore = Chroma(
    collection_name="split_parents", embedding_function=watsonx_embedding
)

# Set up an in-memory storage layer for the parent documents
# This will store the larger chunks that provide context, but won't be directly embedded
store = InMemoryStore()

# Create a ParentDocumentRetriever instance that implements hierarchical document retrieval
retriever = ParentDocumentRetriever(
    # The vector store where child document embeddings will be stored and searched
    # This Chroma instance will contain the embeddings for the smaller chunks
    vectorstore=vectorstore,
    
    # The document store where parent documents will be stored
    # These larger chunks won't be embedded but will be retrieved by ID when needed
    docstore=store,
    
    # The splitter used to create small chunks (400 chars) for precise vector search
    # These smaller chunks are embedded and used for similarity matching
    child_splitter=child_splitter,
    
    # The splitter used to create larger chunks (2000 chars) for better context
    # These parent chunks provide more complete information when retrieved
    parent_splitter=parent_splitter,
)
# Then, we add documents to the hierarchical retrieval system:

retriever.add_documents(document)
# The following code retrieves and counts the number of parent document IDs stored in the document store
len(list(store.yield_keys()))

```

Next, we verify that the underlying vector store still retrieves the small chunks.

```bash
sub_docs = vectorstore.similarity_search("Langchain")
print(sub_docs[0].page_content)

#And then retrieve the relevant large chunk.
retrieved_docs = retriever.invoke("Langchain")
print(retrieved_docs[0].page_content)
```

### **RetrievalQA**

Now that you understand how to retrieve information from a document, you might be interested in exploring some more exciting applications. For instance, you could have the Language Model (LLM) read the paper and summarize it for you, or create a QA bot that can answer your questions based on the paper.

Here's an example using LangChain's `RetrievalQA`.

```bash
from langchain.chains import RetrievalQA

# Create a RetrievalQA chain by configuring:
qa = RetrievalQA.from_chain_type(
    # The language model to use for generating answers
    llm=llama_llm,
    
    # The chain type "stuff" means all retrieved documents are simply concatenated and passed to the LLM
    chain_type="stuff",
    
    # The retriever component that will fetch relevant documents
    # docsearch.as_retriever() converts the vector store into a retriever interface
    retriever=docsearch.as_retriever(),
    
    # Whether to include the source documents in the response
    # Set to False to return only the generated answer
    return_source_documents=False
)

# Define a query to test the QA system
# This question asks about the main topic of the paper
query = "what is this paper discussing?"

# Execute the QA chain with the query
# This will:
# 1. Send the query to the retriever to get relevant documents
# 2. Combine those documents using the "stuff" method
# 3. Send the query and combined documents to the Llama LLM
# 4. Return the generated answer (without source documents)
qa.invoke(query)
```

## Exercise -4

```bash
from langchain_core.documents import Document
from langchain_community.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_ibm import WatsonxEmbeddings
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from langchain.chains import RetrievalQA

# 1. Load a document about AI
loader = WebBaseLoader("https://python.langchain.com/v0.2/docs/introduction/")
documents = loader.load()

# 2. Split the document into chunks
text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=20, separator='\n')
chunks = text_splitter.split_documents(documents)

# 3. Set up the embedding model. (Use an embedding model to create vector representations.)
embed_params = {
    EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 3,
    EmbedTextParamsMetaNames.RETURN_OPTIONS: {"input_text": True},
}

embedding_model = WatsonxEmbeddings(
    model_id="ibm/slate-125m-english-rtrvr-v2",
    url="https://us-south.ml.cloud.ibm.com",
    project_id="skills-network",
    params=embed_params,
)  
# use ibm/slate-125m-english-rtrvr-v2 model

# 4. Create a vector store

vector_store = Chroma.from_documents(chunks, embedding_model)
# 5. Create a retriever
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

# 6. Define a function to search for relevant information
def search_documents(query, top_k=3):
    """Search for documents relevant to a query"""
    # Use the retriever to get relevant documents
    docs = retriever.get_relevant_documents(query)
    
    # Limit to top_k if specified
    return docs[:top_k]

# 7. Test with a few queries
test_queries = [
    "What is LangChain?",
    "How do retrievers work?",
    "Why is document splitting important?"
]

for query in test_queries:
    print(f"\nQuery: {query}")
    results = search_documents(query)
    print(f"{len(results)} Results Found")
    for i, doc in enumerate(results):
        print(f"\nResult {i+1}: {doc.page_content[:150]}...")
        print(f"Source: {doc.metadata.get('source', 'Unknown')}")

```

## **Memory**

Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At a bare minimum, a conversational system should be able to directly access some window of past messages.

### **Chat message history**

One of the core utility classes underpinning most (if not all) memory modules is the `ChatMessageHistory` class. This class is a super lightweight wrapper that provides convenience methods for saving `HumanMessages` and `AIMessages`, and then fetching both types of messages.

Here is an example.

```bash
# Import the ChatMessageHistory class from langchain.memory
from langchain.memory import ChatMessageHistory

# Set up the language model to use for chat interactions
chat = llama_llm

# Create a new conversation history object
# This will store the back-and-forth messages in the conversation
history = ChatMessageHistory()

# Add an initial greeting message from the AI to the history
# This represents a message that would have been sent by the AI assistant
history.add_ai_message("hi!")

# Add a user's question to the conversation history
# This represents a message sent by the user
history.add_user_message("what is the capital of France?")
```

```bash
history.messages

[AIMessage(content='hi!'),
 HumanMessage(content='what is the capital of France?')]
 
ai_response = chat.invoke(history.messages)
ai_response

history.add_ai_message(ai_response)
history.messages

```

### **Conversation buffer**

Conversation buffer memory allows for the storage of messages, which you use to extract messages to a variable. Consider using conversation buffer memory in a chain, setting `verbose=True` so that the prompt is visible.

```bash
# Import ConversationBufferMemory from langchain.memory module
from langchain.memory import ConversationBufferMemory

# Import ConversationChain from langchain.chains module
from langchain.chains import ConversationChain

# Create a conversation chain with the following components:
conversation = ConversationChain(
    # The language model to use for generating responses
    llm=llama_llm,
    
    # Set verbose to True to see the full prompt sent to the LLM, including memory contents
    verbose=True,
    
    # Initialize with ConversationBufferMemory that will:
    # - Store all conversation turns (user inputs and AI responses)
    # - Append the entire conversation history to each new prompt
    # - Provide context for the LLM to generate contextually relevant responses
    memory=ConversationBufferMemory()
)

conversation.invoke(input="Hello, I am a little cat. Who are you?")
```

## Exercise - 5

```bash
from langchain.memory import ConversationBufferMemory, ChatMessageHistory
from langchain.chains import ConversationChain
from langchain_core.messages import HumanMessage, AIMessage
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM

# 1. Set up the language model
model_id = 'meta-llama/llama-4-maverick-17b-128e-instruct-fp8'
parameters = {
    GenParams.MAX_NEW_TOKENS: 256,
    GenParams.TEMPERATURE: 0.2,
}
credentials = {"url": "https://us-south.ml.cloud.ibm.com"}
project_id = "skills-network"

# Initialize the model
model = ModelInference(
    model_id=model_id,
    credentials=credentials,
    project_id=project_id,
    params=parameters,
)  
llm = WatsonxLLM(model = model)

# 2. Create a simple conversation with chat history
history = ChatMessageHistory()

# Add some initial messages (optional)
history.add_user_message("Hello, my name is Bhanu Pradeep")
history.add_ai_message("Hello Alice how May I help you")
##TODO: Add an AI response

# 3. Print the current conversation history
##TODO: Print the current messages in history
print(f"Initial Messages : {history.messages}")

for message in history.messages:
    sender = "Human" if isinstance(message,HumanMessage) else "AI"
    print(f"{sender} : {message.content}")

# 4. Set up a conversation chain with memory
memory = ConversationBufferMemory(chat_memory  = history)

conversation = ConversationChain(
    # The language model to use for generating responses
    llm=llama_llm,
    
    # Set verbose to True to see the full prompt sent to the LLM, including memory contents
    verbose=True,
    
    # Initialize with ConversationBufferMemory that will:
    # - Store all conversation turns (user inputs and AI responses)
    # - Append the entire conversation history to each new prompt
    # - Provide context for the LLM to generate contextually relevant responses
    memory=memory
)

# 5. Function to simulate a conversation
def chat_simulation(conversation, inputs):
    """Run a series of inputs through the conversation chain and display responses"""
    print("\n=== Beginning Chat Simulation ===")
    
    for i, user_input in enumerate(inputs):
        print(f"\n--- Turn {i+1} ---")
        print(f"Human: {user_input}")
        
        # Get response from the conversation chain
        response = conversation.invoke(input=user_input)
        
        # Print the AI's response
        print(f"AI: {response['response']}")
    
    print("\n=== End of Chat Simulation ===")

# 6. Test with a series of related questions
test_inputs = [
    "My favorite color is blue.",
    "I enjoy hiking in the mountains.",
    "What activities would you recommend for me?",
    "What was my favorite color again?",
    "Can you remember both my name and my favorite color?"
]

chat_simulation(conversation, test_inputs)

# 7. Examine the conversation memory
print("\nFinal Memory Contents:")
##TODO: Print the contents of the conversation memory
print(conversation.memory.buffer)

# 8. Create a new conversation with a different type of memory (optional)
# Try implementing ConversationSummaryMemory or another type of memory

from langchain.memory import ConversationSummaryMemory

summary_memory  = ConversationSummaryMemory(llm = llm)

summary_memory.save_context(
    {"Input": "hello I am Bhanu"},
    {"Output":"This is ur Assistant Let me know How can I help you to Improve"})

summary_conversation = ConversationChain(
    llm = llm,
    memory= summary_memory,
    verbose= True
)

print("\n ===========Testing Conversation summary memory===============")

chat_simulation(summary_conversation,test_inputs)

print("\n ===========Conversation summary buffer===============")
print(summary_conversation.memory.buffer)

print("\n ==============Memory Comparision ====================")

print(f" summary conversation : {len(summary_conversation.memory.buffer)} contents")
print(f" Buffer conversation : {len(conversation.memory.buffer)} contents")

```

### **Chains**

`Chains` are one of the most powerful features in LangChain, allowing you to combine multiple components into cohesive workflows. This section presents two different methodologies for implementing chains - the traditional `SequentialChain` approach and the newer LangChain Expression Language (`LCEL`).

**Why Chains Matter:**

Chains solve a fundamental problem with LLMs. Chains are primarily designed to handle a single prompt and generate a single response. However, most real-world applications require multi-step reasoning, accessing different tools, or breaking complex tasks into manageable pieces. Chains allow you to orchestrate these complex workflows.

**Evolution of Chain Patterns:**

Traditional chains (`LLMChain`, `SequentialChain`) were LangChain's first implementation, offering a structured but somewhat rigid approach. LCEL (using the pipe operator `|`) represents a more flexible, functional approach that's easier to compose and debug.

**Note:** While both approaches are presented here for educational purposes, **LCEL is the recommended pattern for new development.** The SequentialChain approach continues to be supported for backward compatibility, but the LangChain community has largely transitioned to the LCEL pattern for its superior flexibility and expressiveness.

**Simple Chain**

**Traditional Approach: LLMChain**

Here is a simple single chain using `LLMChain`.

```bash
# Import the LLMChain class from langchain.chains module
from langchain.chains import LLMChain

# Create a template string for generating recommendations of classic dishes from a given location
# The template includes:
# - Instructions for the task (recommending a classic dish)
# - A placeholder {location} that will be replaced with user input
# - A format indicator for the expected response
template = """Your job is to come up with a classic dish from the area that the users suggests.
{location}
 YOUR RESPONSE:
"""

# Create a PromptTemplate object by providing:
# - The template string defined above
# - A list of input variables that will be used to format the template
prompt_template = PromptTemplate(template=template, input_variables=['location'])

# Create an LLMChain that connects:
# - The Llama language model (llama_llm)
# - The prompt template configured for location-based dish recommendations
# - An output_key 'meal' that specifies the key name for the chain's response in the output dictionary
location_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='meal')

# Invoke the chain with 'China' as the location input
# This will:
# 1. Format the template with {location: 'China'}
# 2. Send the formatted prompt to the Llama LLM
# 3. Return a dictionary with the response under the key 'meal'
location_chain.invoke(input={'location':'Andhra pradesh'})
```

```bash
# Import PromptTemplate from langchain_core.prompts
# This is the new import path in LangChain's modular structure
from langchain_core.prompts import PromptTemplate

# Import StrOutputParser from langchain_core.output_parsers
from langchain_core.output_parsers import StrOutputParser

template = """Your job is to come up with a classic dish from the area that the users suggests.
{location}
 YOUR RESPONSE:
"""

# Create a prompt template using the from_template method
prompt = PromptTemplate.from_template(template)

# Create a chain using LangChain Expression Language (LCEL) with the pipe operator
# This creates a processing pipeline that:
# 1. Formats the prompt with the input values
# 2. Sends the formatted prompt to the Llama LLM
# 3. Parses the output to extract just the string response
location_chain_lcel = prompt | llama_llm | StrOutputParser()

# Invoke the chain with 'China' as the location
result = location_chain_lcel.invoke({"location": "Andhra pradesh"})

# Print the result (the recommended classic dish from China)
print(result)
```

### **Simple sequential chain**

Sequential chains allow you to use output of one LLM as the input for another LLM. This approach is beneficial for dividing tasks and maintaining the focus of your LLM.

In this example, you see a sequence that:

- Gets a meal from a location
- Gets a recipe for that meal
- Estimates the cooking time for that recipe

This pattern is incredibly valuable for breaking down complex tasks into logical steps, where each step depends on the output of the previous step. The traditional approach uses `SequentialChain`, while the modern `LCEL` approach uses piping and `RunnablePassthrough.assign`.

### **Traditional Approach: `SequentialChain`[¶](https://jupyterlabnext-0-labs-prod-jupyterlab-us-east-0.labs.cognitiveclass.ai/user/bhanupradee1/lab/tree/AI0220EN/Build%20Smarter%20AI%20Apps%20Empower%20LLMs%20with%20LangChain.ipynb#Traditional-Approach:-SequentialChain)**

```bash
# Import SequentialChain from langchain.chains module
from langchain.chains import SequentialChain

# Create a template for generating a recipe based on a meal
template = """Given a meal {meal}, give a short and simple recipe on how to make that dish at home.
 YOUR RESPONSE:
"""

# Create a PromptTemplate with 'meal' as the input variable
prompt_template = PromptTemplate(template=template, input_variables=['meal'])

# Create an LLMChain (chain 2) for generating recipes
# The output_key='recipe' defines how this chain's output will be referenced in later chains
dish_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='recipe')
```

```bash
# Create a template for estimating cooking time based on a recipe
# This template asks the LLM to analyze a recipe and estimate preparation time
template = """Given the recipe {recipe}, estimate how much time I need to cook it.
 YOUR RESPONSE:
"""

# Create a PromptTemplate with 'recipe' as the input variable
prompt_template = PromptTemplate(template=template, input_variables=['recipe'])

# Create an LLMChain (chain 3) for estimating cooking time
# The output_key='time' defines the key for this chain's output in the final result
recipe_chain = LLMChain(llm=llama_llm, prompt=prompt_template, output_key='time')
```

```bash
# Create a SequentialChain that combines all three chains:
# 1. location_chain (from earlier code): Takes a location and suggests a dish
# 2. dish_chain: Takes the suggested dish and provides a recipe
# 3. recipe_chain: Takes the recipe and estimates cooking time
overall_chain = SequentialChain(
    # List of chains to execute in sequence
    chains=[location_chain, dish_chain, recipe_chain],
    
    # The input variables required to start the chain sequence
    # Only 'location' is needed to begin the process
    input_variables=['location'],
    
    # The output variables to include in the final result
    # This makes the output of each chain available in the final result
    output_variables=['meal', 'recipe', 'time'],
    
    # Whether to print detailed information about each step
    verbose=True
)
```

```bash
from pprint import pprint
pprint(overall_chain.invoke(input={'location':'Andhra pradesh'}))
```

```bash
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Define the templates for each step
location_template = """Your job is to come up with a classic dish from the area that the users suggests.
{location}

YOUR RESPONSE:
"""

dish_template = """Given a meal {meal}, give a short and simple recipe on how to make that dish at home.

YOUR RESPONSE:
"""

time_template = """Given the recipe {recipe}, estimate how much time I need to cook it.

YOUR RESPONSE:
"""

# Create the location chain using LCEL (LangChain Expression Language)
# This chain takes a location and returns a classic dish from that region
location_chain_lcel = (
    PromptTemplate.from_template(location_template)  # Format the prompt with location
    | llama_llm                                    # Send to the LLM
    | StrOutputParser()                              # Extract the string response
)

# Create the dish chain using LCEL
# This chain takes a meal name and returns a recipe
dish_chain_lcel = (
    PromptTemplate.from_template(dish_template)      # Format the prompt with meal
    | llama_llm                                    # Send to the LLM
    | StrOutputParser()                              # Extract the string response
)

# Create the time estimation chain using LCEL
# This chain takes a recipe and returns an estimated cooking time
time_chain_lcel = (
    PromptTemplate.from_template(time_template)      # Format the prompt with recipe
    | llama_llm                                    # Send to the LLM
    | StrOutputParser()                              # Extract the string response
)

# Combine all chains into a single workflow using RunnablePassthrough.assign
# RunnablePassthrough.assign adds new keys to the input dictionary without removing existing ones
overall_chain_lcel = (
    # Step 1: Generate a meal based on location and add it to the input dictionary
    RunnablePassthrough.assign(meal=lambda x: location_chain_lcel.invoke({"location": x["location"]}))
    # Step 2: Generate a recipe based on the meal and add it to the input dictionary
    | RunnablePassthrough.assign(recipe=lambda x: dish_chain_lcel.invoke({"meal": x["meal"]}))
    # Step 3: Estimate cooking time based on the recipe and add it to the input dictionary
    | RunnablePassthrough.assign(time=lambda x: time_chain_lcel.invoke({"recipe": x["recipe"]}))
)
# Run the chain
result = overall_chain_lcel.invoke({"location": "China"})
pprint(result)
```

## **Exercise 6**

### **Implementing Multi-Step Processing with Different Chain Approaches**

In this exercise, you'll create a multi-step information processing system using both traditional chains and the modern LCEL approach. You'll build a system that analyzes product reviews, extracts key information, and generates responses based on the analysis.

**Instructions:**

1. Import the necessary components for both traditional chains and LCEL.
2. Implement a three-step process using both traditional SequentialChain and modern LCEL approaches.
3. Create templates for sentiment analysis, summarization, and response generation.
4. Test your implementations with sample product reviews.
5. Compare the flexibility and readability of both approaches.
6. Document the advantages and disadvantages of each method.

**Starter code: provide your solution in the TODO parts**